Chapter 1: Introducción al aprendizaje reforzado. 

 Un agente puede familiarizarse con una amplia gama de tareas asumiendo que los problemas
 se pueden modelar con un entorno de trabajo (Framework) que contenga acciones, un ambiente,
 un agente. 

 No es solamente un algoritmo prediciendo un target, si no también es manipular un agente
 en un ambiente, donde ese agente tiene un conjunto de acciones que puede escoger para 
 lograr una meta/recompensa.

Historia de aprendizaje reforzado. 
 
  RL es una rama del control óptimo, donde se describe un problema donde se intenta
  lograr un cierto criterio "óptimo" y que leyes  de "control" se necesitan para lograr 
  este fin, tipicamente se define un control óptimo como un conjunto de ecuaciones
  diferenciales.
 
      Markov Decision processes (MDPs) y su relación con el RL
 
      MDPs es un proceso de control estocástico discreto, están especificamente diseñadas
      para situaciones en cuales las salidas son parcialmente afectadas por los participantes
      en el proceso pero el proceso también muestra algún grado de aleatorización también.
      
      MDPs y la programación dinámica se vuelven la base de la teoría del aprendizaje reforzado.
      
      Se asume basado a la propiedad de Markov que el futuro es independiente del pasado dado el
      presente, esto quiere decir que el estado actual es la única pieza de información que 
      será relevante y la infomación pasada no será mas necesaria.
      
      En este caso considermos tuplas de la forma (S,P) en el espacio de estados (state) donde
      los estados cambian mediante una función de transición P 
      
      P define una distribución de probabilidad donde la distribución esta definida en todos los
      posibles estados que el agente puede estar. 
      
      Finalmente, tenemos una recompensa que recibimos de movernos de un estado a otro, donde
      se define matemáticamente como el valor esperado de una función de recompensa (R), y gamma 
      una factor de descuento. 
      
      para definir un proceso de recompensa de markov (MRP) como la tupla (S,P,R,gamma)
      
      Otro componente en el desarrollo del aprendizaje reforzado era el aprendiza por
      ensayo y error.
  
Algoritmos del RL y RL frameworks.
  
  Rl es análogomante es muy similar al  dominio del aprendizaje supervizado del machine 
  learning, aunque hay diferencias importantes:
     
     En el aprendizaje supervizado tenemos una respuesta donde entrenamos un modelo para
     predecir correctamente, sea una categoría o un valor en particular basado en los
     features de unas observaciones. 
     
     los features son análogos a los vectores en un estado dado dentro un ambiente,
     con el cual alimentamos al algoritmo de RL, tipicamente como una serie de estados
     o individualmente de un estado a otro. 
     
   La diferencia mas importante es que no necesariamente tenemos una respuesta para
   resolver un problema particular, existen multiples forma por el cual un algoritmo de
   RL puede resolver un problema. En esta instancia, se quiere  escoger la respuesta 
   que resuelva el problema de la manera mas eficiente posible, en este
   aspecto es donde la escogencia del modelo se vuelve crítica. 
   
   Se discutirá el rl framework OpenAI Gym y como interactua con diferentes frameworks
   de Deep Learning
   
   Q Learning
   
   Q Learning está caracterizado por el hecho de que hay algún policia el cual informa
   a un agente de las acciones que debe tomar en diferentes escenarios, como no requiere
   un modelo, podemos usar uno y es especificamente a menudo aplicado a procesos decision 
   markovianos finitos
   
   
    Inicializar Q-table -> Escoger una acción -> realizar una acción -> medir la recompensa -> actualizar Q-table -> Escoger una acción... 
    
#############################################################################  
Chapter 2: Algortimos del aprendizaje reforzado 
    
      Open AI gym
          
          las bases del gym es el ambiente, se tratará el problema del 
          cart pole, que trata acerca de mantener el palo balanceado en el carro,
          recibiremos una unidad de recompensa por cada recuadro (frame) en el cual
          el palo está en posición vertical, el juego se pierde si el palo no vuelve
          a estar vertical en ningún recuadro.
          
          Se enforcará en los metodos que realizaron para resolver este problema, enfatizando 
          en el uso de metodos del grandiente (policy gradient methods)
          
       Policy-Based Learning
          
          Policy-based gradient methods se enfocan en optimizar la función regla
          directamente mas que intentar aprender de una función que producirá
          información en las recompensas esperadas dado un estado.
          
          *Deterministicos: es una regla que mapea un estado dado con una acción
          especificamente en donde las acciones tomadas determinan cual será el 
          resultado, por ejemplo: si estoy escribiendo en el teclado, estoy 
          seguro que si oprimo la tecla "y" aparecerá el caracter y en la pantalla
          
          *Estocastico: una regla que produce una distribución de probabilidad sobre
          un conjunto de acciones, como que existe una probabilidad de que la acción
          tomada no sea la acción que en verdad ocurre.
          
          
          Diferencias entre Policy-based methods y value-function method: 
             
             1. PGM Tienden a converger en mejores soluciones que VFM por que
             en PGM estamos buscando acciones que minimicen la función de error
             en contraste en los VFM se obtiene un amplio y no intuitivo rangos
             de valores de acciones de minima diferencia* 
             
             2. PGM se adaptan en el aprendizaje de procesos estocásticos mientras
             los VFM no.
             
             los VFM requieren unos ambientes explicitos definidos donde las acciones
             dentro de estos ambientes produzcan salidas especificas que deben ser 
             deterministicas
             
             3. PGM son mas efectivos en espacios de alta dimensión, ya que son 
             menos caros computacionalmente hablando, VFM se requiere calcular
             un valor por cada posible accion.
             
             
             Aplicación de PGM al cart pole problem.
             
             
             
            
       
       
          
          
          
  
